{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "id": "ywKyCcp90CI4",
        "outputId": "9a58acca-4f8c-42fa-8f09-69192bcd78bf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "numpy"
                ]
              },
              "id": "22efdad638c948288994ac844577f138"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# 1. Install everything\n",
        "!git clone https://github.com/lm-sys/FastChat.git\n",
        "%cd FastChat\n",
        "!pip install -e .\n",
        "!pip install gradio==4.29.0 torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Download model (example: flan-t5-small)\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/google/flan-t5-small ./flan-t5-small"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSJq3mkz0ND_",
        "outputId": "a7a106d9-a588-419f-d948-39efd29db81e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into './flan-t5-small'...\n",
            "remote: Enumerating objects: 62, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 62 (delta 25), reused 21 (delta 21), pack-reused 34 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (62/62), 615.41 KiB | 4.59 MiB/s, done.\n",
            "Filtering content: 100% (5/5), 1.27 GiB | 66.35 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸš€ 1) Controller\n",
        "!nohup python3 -m fastchat.serve.controller \\\n",
        "  --host 0.0.0.0 \\\n",
        "  --port 21001 > controller.log 2>&1 &"
      ],
      "metadata": {
        "id": "N1z19aQa0Oe2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tail -n 20 controller.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihqKI4QS3_2r",
        "outputId": "8721d910-6bf1-4d9e-a8c9-44a845c33768"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-09 12:27:47 | INFO | controller | args: Namespace(host='0.0.0.0', port=21001, dispatch_method='shortest_queue', ssl=False)\n",
            "2025-09-09 12:27:47 | ERROR | stderr | INFO:     Started server process [1111]\n",
            "2025-09-09 12:27:47 | ERROR | stderr | INFO:     Waiting for application startup.\n",
            "2025-09-09 12:27:47 | ERROR | stderr | INFO:     Application startup complete.\n",
            "2025-09-09 12:27:47 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:21001 (Press CTRL+C to quit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸš€ 2) Model Worker (flan-t5-small)\n",
        "!nohup python3 -m fastchat.serve.model_worker \\\n",
        "  --model-path ./flan-t5-small \\\n",
        "  --device cuda --num-gpus 1 \\\n",
        "  --controller-address http://localhost:21001 > worker.log 2>&1 &"
      ],
      "metadata": {
        "id": "pTJjfsL60RD1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tail -n 20 worker.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvqLQVQr0T4c",
        "outputId": "47fed8b8-374b-470f-c3f4-1cd511eb2cb7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1757420964.695398    1466 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1757420964.701471    1466 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1757420964.716740    1466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757420964.716764    1466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757420964.716766    1466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757420964.716769    1466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-09 12:29:24.721340: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/content/FastChat/fastchat/model/model_chatglm.py:32: SyntaxWarning: invalid escape sequence '\\?'\n",
            "  [\"\\?\", \"ï¼Ÿ\"],\n",
            "2025-09-09 12:29:29 | INFO | model_worker | args: Namespace(host='localhost', port=21002, worker_address='http://localhost:21002', controller_address='http://localhost:21001', model_path='./flan-t5-small', revision='main', device='cuda', gpus=None, num_gpus=1, max_gpu_memory=None, dtype=None, load_8bit=False, cpu_offloading=False, gptq_ckpt=None, gptq_wbits=16, gptq_groupsize=-1, gptq_act_order=False, awq_ckpt=None, awq_wbits=16, awq_groupsize=-1, enable_exllama=False, exllama_max_seq_len=4096, exllama_gpu_split=None, exllama_cache_8bit=False, enable_xft=False, xft_max_seq_len=4096, xft_dtype=None, model_names=None, conv_template=None, embed_in_truncate=False, limit_worker_concurrency=5, stream_interval=2, no_register=False, seed=None, debug=False, ssl=False)\n",
            "2025-09-09 12:29:29 | INFO | model_worker | Loading the model ['flan-t5-small'] on worker ecdfbc82 ...\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "2025-09-09 12:29:29 | INFO | model_worker | Register to controller\n",
            "2025-09-09 12:29:29 | ERROR | stderr | INFO:     Started server process [1466]\n",
            "2025-09-09 12:29:29 | ERROR | stderr | INFO:     Waiting for application startup.\n",
            "2025-09-09 12:29:29 | ERROR | stderr | INFO:     Application startup complete.\n",
            "2025-09-09 12:29:29 | ERROR | stderr | INFO:     Uvicorn running on http://localhost:21002 (Press CTRL+C to quit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸš€ 3) Gradio Web UI (Ù‡ÙŠØ¯ÙŠÙƒ Ù„ÙŠÙ†Ùƒ public ØªÙ„Ù‚Ø§Ø¦ÙŠ)\n",
        "!nohup python3 -m fastchat.serve.gradio_web_server \\\n",
        "  --controller-url http://localhost:21001 \\\n",
        "  --share > gradio.log 2>&1 &"
      ],
      "metadata": {
        "id": "WGK4xoiS2Wim"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tail -n 20 gradio.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4sxfP2d3YjQ",
        "outputId": "301c0ae8-eeb8-4b2e-ab20-aced1a8296d8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-09 12:30:23.838072: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1757421023.859378    1763 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1757421023.865305    1763 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1757421023.879970    1763 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757421023.879996    1763 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757421023.879999    1763 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1757421023.880001    1763 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-09 12:30:23.885253: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-09-09 12:30:27 | INFO | gradio_web_server | args: Namespace(host='0.0.0.0', port=None, share=True, controller_url='http://localhost:21001', concurrency_count=10, model_list_mode='once', moderate=False, show_terms_of_use=False, register_api_endpoint_file=None, gradio_auth_path=None, gradio_root_path=None, use_remote_storage=False)\n",
            "2025-09-09 12:30:27 | INFO | gradio_web_server | All models: ['flan-t5-small']\n",
            "2025-09-09 12:30:27 | INFO | gradio_web_server | Visible models: ['flan-t5-small']\n",
            "2025-09-09 12:30:27 | INFO | stdout | Running on local URL:  http://0.0.0.0:7860\n",
            "2025-09-09 12:30:27 | INFO | stdout | IMPORTANT: You are using gradio version 4.29.0, however version 4.44.1 is available, please upgrade.\n",
            "2025-09-09 12:30:27 | INFO | stdout | --------\n",
            "2025-09-09 12:30:27 | INFO | stdout | Running on public URL: https://043ccba156dccb412e.gradio.live\n",
            "2025-09-09 12:30:27 | INFO | stdout | \n",
            "2025-09-09 12:30:27 | INFO | stdout | This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸš€ 4) OpenAI-Compatible API Server\n",
        "!nohup python3 -m fastchat.serve.openai_api_server \\\n",
        "  --host 0.0.0.0 \\\n",
        "  --port 8000 \\\n",
        "  --controller-address http://localhost:21001 > api_server.log 2>&1 &"
      ],
      "metadata": {
        "id": "0ZtaR7g58Iy_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tail -n 20 api_server.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBPFyt5E3ane",
        "outputId": "b9bd0955-55a2-4470-fdee-007472583b13"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-09 12:30:51 | INFO | openai_api_server | args: Namespace(host='0.0.0.0', port=8000, controller_address='http://localhost:21001', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_keys=None, ssl=False)\n",
            "2025-09-09 12:30:51 | ERROR | stderr | INFO:     Started server process [1931]\n",
            "2025-09-09 12:30:51 | ERROR | stderr | INFO:     Waiting for application startup.\n",
            "2025-09-09 12:30:51 | ERROR | stderr | INFO:     Application startup complete.\n",
            "2025-09-09 12:30:51 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"EMPTY\")\n",
        "\n",
        "resp = client.chat.completions.create(\n",
        "    model=\"flan-t5-small\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"What is AI ?\"}],\n",
        ")\n",
        "\n",
        "print(\"Response:\", resp.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0yupIvi8y5K",
        "outputId": "50114898-a42b-4760-c46b-86fdccbf37fe"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: AI is the name of an artificial intelligence assistant.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8yZzJTbeeroZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}